<?xml version="1.0" encoding="utf-8"?>
<search>


    <entry>
        <title>Adversarial Substructured Representation Learning for Mobile User Profiling 阅读笔记</title>
        <link href="/2022/06/13/Adversarial-Substructured-Representation-Learning-for-Mobile-User-Profiling-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
        <url>
            /2022/06/13/Adversarial-Substructured-Representation-Learning-for-Mobile-User-Profiling-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/
        </url>

        <content type="html">
            <![CDATA[<h1 id="adversarial-substructured-representation-learning-for-mobile-user-profiling">Adversarial Substructured Representation Learning for Mobile User Profiling</h1><blockquote><ul><li>Pengyang Wang，Yanjie Fu，Hui Xiong，Xiaolin Li</li><li>Missouri Univ. of Sci. and Tech.</li><li>Rutgers University</li><li>Nanjing University</li><li><a href="https://dl.acm.org/doi/abs/10.1145/3292500.3330869">KDD 2019</a></li></ul></blockquote><h2 id="背景知识">背景知识</h2><ul><li>移动用户分析是对用户特定移动活动的特征总结。</li><li>移动用户特征分析是指从移动活动中提取用户的兴趣和行为模式，如购物和通勤。</li><li>之前的研究可分为：<ol type="1"><li>明确的特征提取。通过明确定义并从移动行为数据（如人口统计学、网站点击、移动购买、应用内行为）中提取的内容特征来分析用户。这样的分析方法高度依赖于对全面的用户相关信息的收集。</li><li>隐性轮廓学习，包括协作方法、潜在因素模型、网络嵌入和深度学习。<ul><li>特别是，协作方法假设同一群体的用户行为相似，因此，共享相似的分析。这类方法受到目标用户的同伴信息稀少的影响。</li><li>潜在因素模型，如矩阵/张量因子化或基于主题建模的变体，被开发用来将用户资料建模为潜伏因子或分类分布表示。这类方法通过大参数空间的优化来学习用户特征，因此，很容易过度拟合。它高度需要领域知识启发的模型正则化。</li></ul></li></ol></li><li>人类的活动，如购物、上学、工作、吃饭、旅行、娱乐，都是有空间、时间和社会结构的。我们如何确定一个数据结构来更好地描述一个移动用户的活动？传统的方法提取基于内容的特征向量，并不足以解决这个问题。</li></ul><h2 id="本文方法">本文方法</h2><h3 id="方法描述">方法描述</h3><p>我们知道，一个网页的点击率在很大程度上取决于网页内容和网页结构。同样，如果我们把一个移动用户看作是一个网页，那么用户的活动就可以看作是网页内容，而用户的活动转换模式就可以看作是网页结构。图被广泛用于描述结构性和关系性的知识。一个移动用户的资料确实是一个固有的相互联系的活动组成，并且可以很容易地被建模为一个图。</p><p>我们建议构建一个用户活动图来描述每个移动用户，其中顶点是活动类型（即POI类别），边是活动（POI类别）之间的转换频率。通过这种方式，我们将移动用户分析重新表述为从用户活动图中学习用户的深度表征问题。</p><p>在研究了许多用户活动图之后，我们发现了另一种重要的结构信息，我们称之为子结构。子结构指的是具有特定拓扑结构的子图。这种子结构确实表明了移动用户独特的个性化活动模式，并暗示了用户的社会属性和偏好。例如，年轻人喜欢在工作综合体、餐馆和电影院之间中转，而企业家喜欢在商业广场之间中转。</p><p>在本文中，我们重点关注用户活动图的两个子结构：高频顶点和环。</p><blockquote><p>例子：</p><figure><img src="https://s2.loli.net/2022/06/13/MrQeo6DTFmuApO2.png" alt="图1" /><figcaption aria-hidden="true">图1</figcaption></figure><p>图1显示了两个用户活动图的例子，它们是从用户的移动签到事件集中提取的<code>&#123;&lt;事件ID，用户ID，日期，POI类别，经度，纬度&gt; &#125;</code>。红色环显示了用户在工作日中经常在不同的POI类别之间转换，青色的独立顶点是该用户在工作期间经常访问的。不同的用户从他们的活动图中显示出不同的子结构。</p><p>显然，图1(a)显示了一个有小孩的教师的活动图，其中用户在家里、办公室和幼儿园之间转换，并在工作期间经常访问与大学有关的POI类别；而图1(b)显示了一个金融专业人士的活动图，其中用户在家里、商业广场和餐馆之间转换，并在工作后经常访问健身房和咖啡馆。</p></blockquote><p>如何将子结构模式整合到用户活动图的表示学习中？我们非常需要一个统一的学习框架来对整个图和子结构信息共同建模。</p><p>现有方法的一些问题：一个直观的方法是使用Embedding技术，例如自动编码器，来学习整个图的表示。然后，子结构信息被表述为优化目标的正则化项。然而，如图1所示，不同用户的活动子结构具有不同的拓扑结构（高频顶点，环）；这些用户的活动子结构动态地分布在图的不同位置。损失函数中的正则化项无法解决训练中的这些挑战（不同的拓扑结构和分布）。</p><p>解决方法：<strong>生成式对抗网络的出现为解决这一问题提供了巨大的潜力。我们建议将子结构的整合转化为对抗性子结构学习范式。这个范式包括一个自动编码器，用于保留整个图的结构；一个子结构检测器，用于检测图中的子结构；一个对抗训练器，用于通过对抗性攻击纳入子结构的正则化。</strong>所提出的对抗性子结构学习范式似乎可以从战略上解决这个问题。学习范式可以战略性地解决这些挑战。但是，我们后来发现，如果我们使用传统的子图检测算法（例如，DFS）来作为检测器，<strong>这些检测算法通常是不可微的，因此无法对梯度就行反向传播</strong>。为了解决这个问题，我们建议预先训练一个卷积神经网络来捕捉子结构的模式，以接近传统的不可微的子结构算法。</p><p>在本文中，我们开发了一个用于移动用户分析的对抗性子结构学习框架。</p><p>贡献如下：</p><ol type="1"><li>我们创建用户活动图来描述移动用户的特征、模式和偏好。</li><li>我们将移动用户分析重新表述为学习用户活动图的深度表示的问题。</li><li>我们确定了另一个结构信息：用户活动图中的子结构，并开发了一个对抗性的子结构学习范式，包括一个自动编码器、一个检测器和一个对抗性训练器，以保留整个图和子结构信息。</li><li>我们预先训练了一个卷积神经网络（CNN）来近似于传统的子图检测算法，以解决不可微的问题。</li><li>我们将用户分析的结果应用于下一个活动类型的预测，并提出了大量的实验，以证明所提出的方法在现实世界中的增强性能。所提出的方法在真实世界的移动签到数据中的增强性能。</li></ol><h3 id="前置知识点">前置知识点</h3><ol type="1"><li><p>定义1：<strong>用户活动图/User Activity Graph</strong>。一个移动用户的活动被表示为一个用户活动图，其中顶点是活动类型（即POI类别），边是POI类别之间的转换频率。图1显示用户活动图可以描述用户活动的行为结构信息。</p></li><li><p>定义2：<strong>整图结构/Structure of the entire graph</strong>。给定一个用户活动图<span class="math inline">\(G=(V,E)\)</span>，其中V是顶点集，E是边集，整图结构被定义为整个图的全局拓扑表示。整图结构保留了顶点和边之间的关系。对于移动用户，整图结构可以捕获所有POI类别的一般偏好模式。</p></li><li><p>定义3：<strong>子图结构（子结构）/Structure of the subgraph</strong>。其被定义为子图的拓扑表示，能以用户活动的独特行为模式为特征。在本文中，我们重点关注两种类型的子结构。(1) 高频顶点，其累计访问频率高于预先定义的阈值；(2) 环。具体来说，用户活动图中的高频顶点代表对特定类型活动的个性化偏好；用户活动图中的环代表对闭环连续活动模式的个性化偏好。高频顶点和环都可以暗示用户在日常生活中的独特活动模式。</p></li><li><p>定义4：<strong>问题陈述/Problem Statement</strong>。在本文中，我们研究了利用移动活动数据学习用户特征的问题。我们的目标是自动学习一个分析向量来代表一个用户的活动模式。我们从移动活动数据，即POI签到数据中提取用户活动图，以代表移动用户的活动模式。因此，我们把这个问题表述为一个学习用户活动图的深度表征的任务。虽然用户活动图展示了一个用户的整体活动情况，但用户的独特活动模式通常由活动图的各种子结构所暗示。因此，这项任务是一个共同的目标，即在用户活动图中同时保留整体活动模式和在表征学习中保存用户的子结构模式。</p><p>从形式上看，给定一组用户活动图，我们的目标是找到一个映射函数<span class="math inline">\(f:x\rightarrow z\)</span>，该函数将用户活动图x作为输入，并输出用户的矢量表示z，同时遵守整图结构和子结构的保存约束。</p></li></ol><h3 id="框架概览">框架概览</h3><figure><img src="https://s2.loli.net/2022/06/13/6jvfdPrEkZG7pHS.png" alt="图2" /><figcaption aria-hidden="true">图2</figcaption></figure><p>图2显示了我们提出的框架的概况，其中包括以下基本任务：</p><ul><li><p>构建用户活动图来表示移动用户的概况；</p></li><li><p>开发一个对抗性的子结构学习框架，从用户活动图中学习用户表征；</p></li><li><p>在下一个活动类型预测的应用中评估学到的用户表征。在第一个任务中，给定用户的移动签到序列，我们为每个用户构建一个用户活动图。在第二个任务中，我们开发了对抗性子结构学习框架，其共同目标是：</p><ul><li><p>对用户活动图的整图结构进行建模；</p></li><li><p>构建一个可微分的子结构检测器；</p></li><li><p>利用对抗性训练将子结构正则化纳入表征学习。</p></li></ul><p>在最后一项任务中，我们应用我们提出的方法对移动用户进行剖析，以进行下一个活动类型的预测。</p></li></ul><h3 id="对抗性子结构学习">对抗性子结构学习</h3><figure><img src="https://s2.loli.net/2022/06/13/2l46mup9MvxW1Aq.png" alt="图3" /><figcaption aria-hidden="true">图3</figcaption></figure><h4 id="模型直觉">模型直觉</h4><ol type="1"><li>直觉1：<strong>整图结构保留</strong>。活动图的整图结构代表了用户活动之间的相互作用。这种互动可以是强链接、弱链接或无链接。因此，我们应该保留全局行为模式。</li><li>直觉2：<strong>子结构保留</strong>。活动图中有一些独特的子结构，如高频活动和活动过渡环，它们可以独特地描述用户的概况。我们应该保留子结构的行为模式。</li><li>直觉3：<strong>整图结构与子结构的整合</strong>。直观地说，我们可以通过网络Embedding对整个结构进行建模，并通过优化正则化捕获子结构。然而，不同的用户在他们的子结构中可能表现出不同的活动类型、拓扑结构和空间分布。我们需要一个新的学习范式来统一整图结构和子结构。</li><li>直觉4：<strong>可微的子结构检测器</strong>。传统的子图检测算法是不可微的。如果这些检测算法被整合到深度学习框架中，就很难应用梯度下降法进行优化。因此，我们需要一个可微分的子结构检测器来接近非可微的检测算法。</li></ol><h4 id="总体思路">总体思路</h4><p>图3显示了我们提出的对抗性子结构学习框架，包括一个深度自动编码器、一个近似的子结构检测器、一个判别器和一个对抗性训练器。自动编码器是为了保留整图结构并推导出图的表示。我们使用传统的子图检测算法来检测子结构标签，然后使用这些标签来预训练一个CNN来接近传统的子图检测算法。鉴别器是对原始图的子结构（真实的子结构集）和重建图的子结构（生成的子结构集）进行分类。对抗训练器通过强制自编码器特别注意保留重建图中的子结构来整合子结构意识，以混淆鉴别器。</p><h4 id="保留整图结构">保留整图结构</h4><p>我们利用深度自动编码器，在表征学习中保留用户的全局行为结构。具体来说，该自动编码器包括一个编码步骤和一个解码步骤。编码步骤将用户活动图作为输入，并输出一个用户特征向量。解码步骤使用用户特征向量来重构用户活动图。用户特征向量通过最小化重建损失来捕捉全局行为结构。</p><p>形式上，对于给定的第<span class="math inline">\(i\)</span>个图，我们通过将每个顶点的邻居连接信息链接成单一向量来扁平化该图，用<span class="math inline">\(x_i\)</span>来表示，这确实捕捉到了图的全局结构。令<span class="math inline">\(y^1,y^2,...,y^o\)</span>分别为编码步骤中隐藏层<span class="math inline">\(1,2,...,o\)</span>的图的潜在特征表示。用户活动图的Embedding表示为一个<span class="math inline">\(d\)</span>维向量，标记为<span class="math inline">\(z_i\in \mathbb{R}^d\)</span>。因此，编码步骤表示为： <span class="math display">\[\begin{cases}y_i^1=\sigma (W^1x_i+b^1)\nonumber\\y_i^k=\sigma (W^ky_i^{k-1}+b^k),\forall k\in\{2,3,...,o\}\nonumber\\z_i=\sigma (W^{o+1}y_i^o+b^{o+1})\nonumber\end{cases}\]</span> 解码器将Embedding表示<span class="math inline">\(z_i\)</span>作为输入，输出一个重建图，标记为<span class="math inline">\(\hat{x}_i\)</span>。每个隐藏层的潜在特征向量为<span class="math inline">\(\hat{y}_i^1,\hat{y}_i^2,...,\hat{y}_i^o\)</span>。解码过程表示为： <span class="math display">\[\begin{cases}\hat{y}_i^o=\sigma (\hat{W}^{o+1}z_i+\hat{b}^{o+1})\nonumber\\\hat{y}_i^{k-1}=\sigma (\hat{W}^k\hat{y}_i^k+\hat{b}^k),\forall k\in\{2,3,...,o\}\nonumber\\\hat{x}_i=\sigma (\hat{W}^1\hat{y}_i^1+\hat{b}^1)\nonumber\end{cases}\]</span> 其中，<span class="math inline">\(W,b\)</span>分别为可学习的权重矩阵和bias。</p><p>最小化原图<span class="math inline">\(x\)</span>和重建图<span class="math inline">\(\hat{x}\)</span>之间的损失，损失函数为： <span class="math display">\[\begin{align}{\cal{L}}_{AE}&amp;=\frac{1}{2}\sum_{i=1}^m||(x_i-\hat{x}_i)||_2^2\end{align}\]</span></p><h4 id="近似子结构检测器">近似子结构检测器</h4><p>传统的子结构检测算法，例如，基于DFS的子图检测，是不可微的。神经网络的梯度不能通过反向传播来传递。因此，我们建议使用预先训练好的卷积神经网络（CNN）来近似传统的子结构检测器。</p><p>形式上，令<span class="math inline">\(F_{detr}\)</span>为传统的子结构检测器，<span class="math inline">\(F_{cnn}\)</span>为CNN子结构检测器。近似过程有以下两个步骤：</p><ol type="1"><li><strong>生成子结构（label）</strong>。使用活动图<span class="math inline">\(x\)</span>作为输入通过<span class="math inline">\(F_{detr}\)</span>生成对应的真实子结构<span class="math inline">\(s_{real}\)</span>作为标注。</li><li><strong>训练<span class="math inline">\(F_{cnn}\)</span>近似<span class="math inline">\(F_{detr}\)</span></strong>。<span class="math inline">\(F_{cnn}\)</span>结构包含两组<code>&#123;Conv, Relu, MaxPooling&#125;</code>，其中卷积核为5，池化核为2。令<span class="math inline">\(\hat{s}\)</span>为<span class="math inline">\(F_{cnn}\)</span>的输出，训练目标是最小化损失：<span class="math inline">\({\cal{L}}_{cnn}=\frac{1}{2}\sum_{i=1}^m||(s_{real}-\hat{s})||_2^2\)</span>。</li></ol><p>最终，我们得到了预先训练好的<span class="math inline">\(F_{cnn}\)</span>作为一个可微分的、近似的子结构检测器。</p><h4 id="通过对抗训练整合子结构信息">通过对抗训练整合子结构信息</h4><p>图3(b)显示，我们开发了一种对抗性学习策略，包括一个生成器、一个鉴别器和一个对抗性训练器，以整合子结构信息。</p><ul><li><p><strong>生成器</strong>：图3(b)显示，生成器将一个深度自动编码器和一个基于预训练的CNN检测器联系起来。具体来说，我们将预先训练好的CNN连接到解码器的最后一层，这样CNN就将解码器输出的重构图<span class="math inline">\(\hat{x}\)</span>作为输入。CNN从重建的图中检测并输出一个子结构，用<span class="math inline">\(\hat{s}\)</span>表示。让G表示生成器，那么映射过程可以表示为：<span class="math inline">\(\hat{s}_i=G(x_i)\)</span>。</p></li><li><p><strong>判别器</strong>：图3(b)显示，判别器为一个多层感知机<span class="math inline">\(D(s;\theta_d)\)</span>，其中<span class="math inline">\(\theta_d\)</span>为其参数，<span class="math inline">\(D(s)\)</span>输出一个概率，表明子结构<span class="math inline">\(s\)</span>来自真实的子结构集<span class="math inline">\(s_{real}\)</span>而不是生成的子结构集<span class="math inline">\(\hat{s}\)</span>的可能性。</p></li><li><p><strong>对抗训练策略</strong>：D的训练是为了最大限度地提高对真实子结构和由G产生的重建子结构的分类精度。G的训练是为了最小化D对由G产生的重建子结构集的分类精度。</p><p>形式上，让<span class="math inline">\(p_{real}(s)\)</span>表示真实子结构集<span class="math inline">\(s_{real}\)</span>的分布，<span class="math inline">\(p_x(x)\)</span>表示原图集<span class="math inline">\(x\)</span>的分布，对抗训练的minimax函数如下： <span class="math display">\[\begin{align}\begin{matrix}min\\G\end{matrix}\,\begin{matrix}max\\D\end{matrix}\,V(D,G)&amp;=\mathbb{E}_{s\sim p_{real}(s)}[\log D(s)]+\mathbb{E}_{x\sim p_x(x)}[\log (1-D(G(x)))]\end{align}\]</span> 特别的，判别器分类准确度为： <span class="math display">\[\begin{align}{\cal{L}}_D&amp;=\frac{1}{m}\sum_{i=1}^m[\log D(s_i)+\log (1-D(G(x_i)))]\end{align}\]</span> 生成器损失为： <span class="math display">\[\begin{align}{\cal{L}}_G&amp;=\frac{1}{m}\sum_{i=1}^m\log(1-D(G(x_i)))\end{align}\]</span> 目标就是最大化损失<span class="math inline">\({\cal{L}}_D\)</span>和最小化损失<span class="math inline">\({\cal{L}}_G\)</span>。</p></li></ul><h4 id="解决优化问题">解决优化问题</h4><p>该模型的损失函数包括。(i) 重建损失最小化（公式1）；(ii) 鉴别器精度最大化（公式3）；以及(iii) 发生器损失最小化（公式4）。其目的是使总体损失<span class="math inline">\(\cal{L}\)</span>最小化，具体如下： <span class="math display">\[\begin{align}{\cal{L}}=-\lambda_D{\cal{L}}_D+\lambda_G{\cal{L}}_G+\lambda_{AE}{\cal{L}}_{AE}\end{align}\]</span> 在<strong>训练阶段</strong>，采用随机梯度下降法来优化，具体来说，先通过下式更新自动编码器： <span class="math display">\[\triangledown_{\theta_{AE}}||(x_i-\hat{x}_i)||_2^2\]</span> 然后在保持预训练检测器<span class="math inline">\(F_{detr}\)</span>参数不变的前提下更新生成器，生成器参数<span class="math inline">\(\theta_g\)</span>与自动编码器参数<span class="math inline">\(\theta_{AE}\)</span>相同，所以通过下式更新生成器： <span class="math display">\[\triangledown_{\theta_{AE}}\frac{1}{m}\sum_{i=1}^m\log(1-D(G(x^i)))\]</span> 然后更新判别器： <span class="math display">\[-\triangledown_{\theta_{AE}}\frac{1}{m}\sum_{i=1}^m[\log D(s^i)+\log(1-D(G(x^i)))]\]</span> 在<strong>测试阶段</strong>，图3(c)显示，我们使用子结构感知编码器来学习用户活动图的表示。</p><h4 id="讨论">讨论</h4><p>在最近的研究中，有一些与保留结构信息的表示学习有关的工作。例如，Wang等人提出在表征学习中对一阶接近性（即仅由边连接的顶点之间的局部成对相似性）和二阶接近性（即顶点的邻域结构的相似性）进行建模。Yu等人提出通过共同考虑保全位置和全局重建的约束，用对抗性正则化自动编码器来捕获网络结构。这些工作与我们的论文的区别在于，我们的工作旨在将子结构（即子图的结构）整合到整图结构（即整个图的结构）中，而之前的这些工作是对局部结构（即邻居的结构）和全局结构的联合建模。</p><h3 id="预测下一个活动类型用户分析">预测下一个活动类型用户分析</h3><p>作为一种应用，我们使用所提出的方法对用户活动图进行剖析，并推断出下一个活动类型，以评估其性能。具体来说，我们首先将一个POI类别视为一种活动类型。然后，对下一个活动类型的偏好由POI类别的访问概率分布来表示，标记为<span class="math inline">\(P^i_{visit}=\{P^{i,1}_{visit},P^{i,2}_{visit},...,P^{i,k}_{visit}\}\)</span>，这实际上是第二天每个POI类别的归一化访问频率。其中，<span class="math inline">\(P^{i,k}_{visit}\)</span>表示用户<span class="math inline">\(u_i\)</span>访问第<span class="math inline">\(k\)</span>个POI类别的概率。</p><p>定义：<strong>下一步要做什么问题</strong>。考虑到用户的历史POI签到记录，我们的目标是通过推断用户在下一天将访问的POI类别的概率来预测用户的下一个活动类型。</p><p>具体来说，对每个用户<span class="math inline">\(u_i\)</span>，首先构建用户活动图<span class="math inline">\(G_i\)</span>，其中一个顶点是一个POI类别的概率，而一条边的权重是访问从一个POI类别到另一个POI类别的频率。然后我们利用我们的方法来学习用户<span class="math inline">\(u_i\)</span>的表征<span class="math inline">\(z_i\)</span>。之后，将<span class="math inline">\(z_i\)</span>作为输入，训练一个全连接神经网络<span class="math inline">\(NN\)</span>，以预测每个用户对下一个活动类型的偏好：<span class="math inline">\(NN:z_i\rightarrow P^i_{visit}\)</span>。最后，我们根据访问概率对POI类别进行排序，生成一个候选列表<span class="math inline">\(R_i\)</span>。</p><h2 id="实验结果">实验结果</h2><h3 id="数据描述">数据描述</h3><p>表1显示了我们来自两个城市的两个签到数据集的统计数据：纽约和东京。每个数据集包括用户ID，场地ID，场地类别ID，场地类别名称，纬度，经度，和时间。</p><table><thead><tr class="header"><th style="text-align: center;">城市</th><th style="text-align: center;"># Check-ins</th><th style="text-align: center;"># POI类别</th><th style="text-align: center;">Time Period</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">纽约</td><td style="text-align: center;">227428</td><td style="text-align: center;">251</td><td style="text-align: center;">12 April 2012 to 16 February 2013</td></tr><tr class="even"><td style="text-align: center;">东京</td><td style="text-align: center;">573703</td><td style="text-align: center;">247</td><td style="text-align: center;">12 April 2012 to 16 February 2013</td></tr></tbody></table><h3 id="评估指标">评估指标</h3><ol type="1"><li><p><span class="math inline">\(Precision@N\)</span>：是指预测用户历史上进行过的下一个预先存在的活动类型的精度。令<span class="math inline">\(\hat{R}_i\)</span>表示根据相应的预测访问概率降序排列的Top-K个预测POI类别列表，<span class="math inline">\(R_i\)</span>表示已访问的POI类别列表，<span class="math inline">\(U\)</span>表示用户集，则<span class="math inline">\(Precision@N\)</span>计算公式如下： <span class="math display">\[Precision@N=\frac{1}{|N|}\sum_{u_i\in U}\frac{|\hat{R}^N_i\cap R_i|}{|R_i|}\]</span></p></li><li><p><span class="math inline">\(Precision^{new}@N\)</span>：是预测用户历史上从未执行过的下一个新活动类型的精度。令<span class="math inline">\(R^{new}_i\)</span>表示用户<span class="math inline">\(u_i\)</span>在训练集中没有访问过但是在测试集中被访问过的POI类别列表。那么，<span class="math inline">\(Precision^{new}@N\)</span>计算公式如下： <span class="math display">\[Precision^{new}@N=\frac{1}{|U|}\sum_{u_i\in U}\frac{|\hat{R}^N_i\cap R^{new}_i|}{|R^{new}_i|}\]</span></p></li></ol><h3 id="模型性能">模型性能</h3><figure><img src="https://s2.loli.net/2022/06/13/ScELqUmWeHk97TO.png" alt="图4" /><figcaption aria-hidden="true">图4</figcaption></figure><p>我们在<span class="math inline">\(Precision@N\)</span>和<span class="math inline">\(Precision^{new}@N\)</span>方面将我们的方法与Baseline方法进行比较。总的来说，图4显示我们的模型在纽约和东京数据集上都取得了最佳性能。在<strong>预测下一个预先存在的活动类型的任务</strong>中，我们观察到当K变大时，我们的方法的准确性比Baseline方法要好得多。在<strong>预测下一个新活动类型的任务</strong>中，我们的方法在发现用户从未访问过的新POI类别方面仍然优于Baseline方法。结果验证了子结构对于用户分析是必不可少的。具体来说，用户活动图的子结构暗示了一些特定的用户活动模式。例如，环子结构可以以特定 POI 类别的活动转换模式为特征，而Node结构可以以用户对多个独立POI类别的偏好为特征。我们提出的框架通过对抗训练有效地考虑了子结构和整图结构。然而，Auto-Encoder、DeepWalk、CNN 和 LINE 无法学习具有子结构意识的表示，从而降低了它们的预测性能。</p><h3 id="鲁棒性检验">鲁棒性检验</h3><p>为了进行鲁棒性检验，我们将我们的方法应用于不同的数据子组，以检查我们的表现的差异性。具体来说，我们将数据集平均分成五个时间段，包括（1）2012年4月12日-2012年6月12日，（2）2012年6月13日-2012年8月13日，（3）2012年8月14日-2012年10月14日，（4）2012年8月15日-2012年10月15日，和（5）2012年10月16日-2013年2月16日。我们将每个时间段的最后一天的活动作为预测目标来进行评估。我们对纽约和东京数据集的预测性能进行了<span class="math inline">\(Precision@N\)</span>和<span class="math inline">\(Precision^{new}@N\)</span>的评估。图5和图6显示，我们的方法可以达到较小的方差，并且相对稳定，特别是当K较小时。</p><figure><img src="https://s2.loli.net/2022/06/13/DhWo6zklKLJ5rjy.png" alt="图5与图6" /><figcaption aria-hidden="true">图5与图6</figcaption></figure><h3 id="子结构保留的研究">子结构保留的研究</h3><figure><img src="https://s2.loli.net/2022/06/13/iNOzoM5LTjwrkyC.png" alt="图7" /><figcaption aria-hidden="true">图7</figcaption></figure><p>我们引入了两种类型的子结构：独立顶点和环。因此，我们研究这两种不同的子结构类型如何影响我们的方法在使用分析上的表现。具体来说，我们表示（1）StructRL-Node：我们框架的一个变体，只考虑离散顶点的子结构；（2）StructRL-环：我们框架的一个变体，只考虑环的子结构；（3）StructRL：我们提出的方法，考虑两者。</p><p>图7显示，StructRL-环的性能总是略微胜过StructRL-Node；换句话说，环的子结构比离散顶点的子结构更有效，可以描述用户的活动模式。环的子结构显示了用户的环活动转换模式，而独立顶点的子结构则显示了用户高度和密集偏好的一些独立POI类别。因此，环的子结构可以描述POI类别之间的相关性，以暗示用户的特定生活方式模式，这比独立顶点的子结构更能提供信息。</p><h3 id="训练损失的研究">训练损失的研究</h3><figure><img src="https://s2.loli.net/2022/06/13/YodzmtUcASIKl9O.png" alt="图8" /><figcaption aria-hidden="true">图8</figcaption></figure><p>图8显示了我们的方法在不同子结构和不同数据集方面的训练损失。重建损失表明了保留图的全局结构模式的有效性。对抗性训练损失表明将子结构整合到整图结构的学习过程。我们可以看到，在同时进行对抗性训练时，重建损失会收敛。换句话说，将子结构信息整合到全局结构中，有助于训练损失的收敛。</p><h2 id="结论">结论</h2><p>我们研究自动移动用户分析的问题。我们将用户表示为活动图，并将用户分析问题重新表述为从用户活动图中学习表示的任务。在分析了众多用户活动图之后，我们发现保留图的整图结构和子结构至关重要。我们观察到，图中子结构的内容、拓扑和位置可以随不同用户动态变化。我们提出了一种对抗性子结构学习方法，在表示学习中联合建模整图结构和子结构（即暗示移动用户独特的个性化活动模式）。具体来说，我们首先采用自动编码通过最小化图重建损失来对整图结构进行建模。此外，我们预训练了一个CNN来逼近不可微分的子结构检测器，因此子结构检测器可以与自动编码配合使用。此外，我们通过将预训练的CNN附加到自动编码器的最后一层来设计生成器，以生成子结构。此外，我们通过对抗性训练整合子结构意识，联合最小化图重建损失和生成器损失并最大化鉴别器损失。我们还将我们的方法应用于预测下一个活动类型的应用。我们用纽约和东京的数据展示了密集的实验结果，以证明我们方法的有效性。</p>]]></content>


        <categories>

            <category>论文笔记</category>

            <category>信息系统</category>

            <category>数据挖掘</category>

        </categories>


        <tags>

            <tag>论文笔记</tag>

            <tag>信息系统</tag>

            <tag>数据挖掘</tag>

        </tags>

    </entry>


    <entry>
        <title>RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality 阅读笔记</title>
        <link href="/2022/05/28/RepMLPNet-Hierarchical-Vision-MLP-with-Re-parameterized-Locality-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
        <url>
            /2022/05/28/RepMLPNet-Hierarchical-Vision-MLP-with-Re-parameterized-Locality-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/
        </url>

        <content type="html">
            <![CDATA[<h1 id="repmlpnet-hierarchical-vision-mlp-with-re-parameterized-locality">RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality</h1><blockquote><ul><li>Xiaohan Ding，Honghao Chen，Xiangyu Zhang，Jungong Han，Guiguang Ding</li><li>Beijing National Research Center for Information Science and Technology (BNRist)</li><li>School of Software, Tsinghua University, Beijing, China</li><li>Institute of Automation, Chinese Academy of Sciences</li><li>MEGVII Technology</li><li>Computer Science Department, Aberystwyth University, SY23 3FL, UK</li><li><a href="https://arxiv.org/abs/2112.11081">CVPR 2022</a></li><li><a href="https://github.com/DingXiaoH/RepMLP">Code</a></li></ul></blockquote><h2 id="背景">背景</h2><p>卷积网络的成功中局部先验发挥着重要的作用，但是传统卷积网络无法很好的处理长距离依赖关系，只能通过加深网络结构缓解问题，而过深的网络导致最终特征图分辨率过小通道数过多反而不利于特征提取。为了解决卷积网络的长距离依赖问题，一些方法采用类似MLP的机制，因为对于全连接层（FC）来说可以获得任意两点之间的关系信息，但是MLP将特征图Flatten的做法会导致位置关系的丢失。</p><p>本文提出了RepMLPNet，其中通过Locality Injection的方法为FC引入位置信息，并且通过结构重参数化机制减少网络在推理时的参数量和计算量，增加推理速度。</p><figure><img src="https://s2.loli.net/2022/05/26/ar5IjCzlgV7AKoO.png" alt="图1" /><figcaption aria-hidden="true">图1</figcaption></figure><hr /><h2 id="method">Method</h2><h3 id="经过重参数化的locality-injection">经过重参数化的Locality Injection</h3><h4 id="基础公式表示">基础公式表示</h4><ul><li><p>对于卷积操作，有张量<span class="math inline">\(M\in \mathbb{R}^{n\times c\times h\times w}\)</span>，其中<span class="math inline">\(n\)</span>表示batch size，<span class="math inline">\(c\)</span>为通道数，<span class="math inline">\(h\)</span>为高度，<span class="math inline">\(w\)</span>为宽度。使用<span class="math inline">\(F\)</span>和<span class="math inline">\(W\)</span>来表示卷积核和FC核。因此对于一个核为<span class="math inline">\(k\times k\)</span>的卷积运算来说有： <span class="math display">\[\begin{align}&amp; M^{(out)}=CONV(M^{(in)},F,p)\nonumber\\\end{align}\]</span> 其中<span class="math inline">\(M^{(out)}\in \mathbb{R}^{n\times o\times h^\prime\times w^\prime}\)</span>，<span class="math inline">\(o\)</span>为卷积输出通道，<span class="math inline">\(p\)</span>指Padding大小，<span class="math inline">\(F\in \mathbb{R}^{o\times c\times k\times k}\)</span>为卷积核。</p></li><li><p>对于FC操作，输入维度为<span class="math inline">\(p\)</span>，输出维度为<span class="math inline">\(q\)</span>，<span class="math inline">\(V^{(in)}\in \mathbb{R}^{n\times p}\)</span>表示输入，<span class="math inline">\(V^{(out)}\in \mathbb{R}^{n\times q}\)</span>表示输出，FC核为<span class="math inline">\(W\in \mathbb{R}^{q\times p}\)</span>，则有矩阵乘法（MMUL）公式： <span class="math display">\[\begin{align}&amp; V^{(out)}=MMUL(V^{(in)},W)=V^{(in)}\cdot W^T\nonumber\\\end{align}\]</span> 假设FC以<span class="math inline">\(M^{(in)}\)</span>作为输入，<span class="math inline">\(M^{(out)}\)</span>作为输出，所以需要Reshape操作（缩写为RS）转换为向量，即<span class="math inline">\(V^{(in)}=RS(M^{(in)},(n,chw))\)</span>，所以上述公式改写为： <span class="math display">\[\begin{align}&amp; M^{(out)}=MMUL(M^{(in)},W)\nonumber\\\end{align}\]</span></p></li></ul><h4 id="locality-injection">Locality Injection</h4><p>如图1所示，通道感知机和局部感知机在训练时是并行的，都使用<span class="math inline">\(M^{(in)}\)</span>作为输入，可以通过如下方法将局部感知机融合进通道感知机中。</p><p>首先，假设FC核为<span class="math inline">\(W(ohw, chw)\)</span>，卷积核为<span class="math inline">\(F(o,c,k,k)\)</span>，Padding大小为<span class="math inline">\(p\)</span>，则可以构建一个同时包含局部感知机和通道感知机的FC操作（核为<span class="math inline">\(W^\prime\)</span>），即 <span class="math display">\[\begin{align}&amp; MMUL(M^{(in)},W^\prime)\nonumber\\&amp; = MMUL(M^{(in)},W)+CONV(M^{(in)},F,p)\\\end{align}\]</span> 又因为对于任意FC核<span class="math inline">\(W^{(1)}\)</span>和<span class="math inline">\(W^{(2)}\)</span>存在： <span class="math display">\[\begin{align}&amp; MMUL(M^{(in)},W^{(1)})+MMUL(M^{(in)},W^{(2)})\nonumber\\&amp; = MMUL(M^{(in)},W^{(1)}+W^{(2)})\\\end{align}\]</span> 所以可以寻找一个FC核<span class="math inline">\(W^{(F,p)}\)</span>使满足： <span class="math display">\[\begin{align}&amp; MMUL(M^{(in)},W^{(F,p)})=CONV(M^{(in)},F,p)\end{align}\]</span> 以替换公式（1）中的卷积操作。</p><p>因为卷积操作可以被看作在空间位置上带有共享权重的稀疏FC，所以对于任意<span class="math inline">\(M^{(in)},F,p\)</span>都存在对应的FC核<span class="math inline">\(W^{(F,p)}\)</span>，因此<strong>问题的关键就在于寻找这个FC核的计算方法</strong>。</p><p>下述方法即为寻找<span class="math inline">\(W^{(F,p)}\)</span>的过程：</p><p>首先，根据“基础公式表示”的FC公式，一个核为<span class="math inline">\(W^{(F,p)}\)</span>的FC的操作即为 <span class="math display">\[\begin{align}&amp; V^{(out)}=V^{(in)}\cdot {W^{(F,p)}}^T\end{align}\]</span> 此时引入一个Identity矩阵<span class="math inline">\(I(chw,chw)\)</span>，得到 <span class="math display">\[\begin{align}&amp; V^{(out)}=V^{(in)}\cdot (I\cdot {W^{(F,p)}}^T)\end{align}\]</span> 同时为了纠正张量尺寸，添加Reshape，有 <span class="math display">\[\begin{align}&amp; V^{(out)}=V^{(in)}\cdot RS(I\cdot {W^{(F,p)}}^T,(chw, ohw))\end{align}\]</span> 可以注意到<span class="math inline">\(I\cdot {W^{(F,p)}}^T\)</span>可通过一个输入特征图从<span class="math inline">\(I\)</span> Reshape到<span class="math inline">\(M^{(I)}\)</span>，核为<span class="math inline">\(F\)</span>的卷积操作得到，即： <span class="math display">\[\begin{align}I\cdot {W^{(F,p)}}^T &amp;=CONV(RS(I,(chw,c,h,w)),F,p)\nonumber\\&amp;=CONV(M^{(I)},F,p)\end{align}\]</span> 因此，综合公式（4）、（6）、（7），核<span class="math inline">\(W^{(F,p)}\)</span>计算推导为 <span class="math display">\[\begin{align}&amp;{W^{(F,p)}}^T=RS(I\cdot {W^{(F,p)}}^T,(chw, ohw))\nonumber\\&amp;\Longrightarrow {W^{(F,p)}}^T=RS(CONV(M^{(I)},F,p),(chw, ohw))\nonumber\\&amp;\Longrightarrow W^{(F,p)}=RS(CONV(M^{(I)},F,p), (chw, ohw))^T\\\end{align}\]</span> 根据上述过程，总结一下就是<strong>一个卷积核等效的FC核是对Identity矩阵进行卷积和适当Reshape的结果</strong>，且该过程可微。</p><h3 id="repmlpnet">RepMLPNet</h3><h4 id="repmlpblock-components">RepMLPBlock Components</h4><figure><img src="https://s2.loli.net/2022/05/26/1oJuxaj4Ts8eRtM.png" alt="图2" /><figcaption aria-hidden="true">图2</figcaption></figure><ul><li><p>全局感知机：输入维度<span class="math inline">\((n,c,h,w)\)</span>经过AvgPool后变为向量<span class="math inline">\((n,c,1,1)\)</span>然后经过两个FC层。</p></li><li><p>通道感知机：如果FC层的输入输出通道相等，那么常规的FC层会产生<span class="math inline">\((chw)^2\)</span>个参数，带来很大的参数量。一个比较自然的想法是参照深度卷积，对每个通道做FC操作，因此只需要计算<span class="math inline">\(c\)</span>个通道的FC，即参数量为<span class="math inline">\(c(hw)^2\)</span>。但是该参数量仍然过大，并且这样做会丢失通道之间的依赖关系，因此本文采用分组共享参数的方式构建“Set-Sharing FC”层。其中，对于输入张量，分为<span class="math inline">\(s\)</span>个Set，每组的多通道共享权重集合，因此参数量减少为<span class="math inline">\(s(hw)^2\)</span>。如图2所示，<span class="math inline">\(n=1,c=4,s=2\)</span>，相当于将输入的4通道划分为2组，每个组有自己的权重集合。</p><p>其具体计算过程为：因为划分为<span class="math inline">\(s\)</span>组，所以有<span class="math inline">\(\frac{c}{s}\)</span>个通道，考虑batch size则有<span class="math inline">\(\frac{nc}{s}\)</span>个维度为<span class="math inline">\((s,h,w)\)</span>的张量，即<span class="math inline">\((n,c,h,w)\Longrightarrow (\frac{nc}{s},s,h,w)\)</span>。将<span class="math inline">\(\frac{nc}{s}\)</span>个张量分别Flatten，得到维度为<span class="math inline">\((\frac{nc}{s}, shw)\)</span>的FC输入张量，然后再对每个张量做FC操作，因此FC操作的参数量为<span class="math inline">\(s\cdot (hw)^2 = s(hw)^2\)</span>。但是该方法与参照深度卷积的FC操作相比并不能减少计算量。实际操作时，“Set-Sharing FC”将<span class="math inline">\((\frac{nc}{s}, shw)\)</span> Reshape为<span class="math inline">\((\frac{nc}{s}, shw,1,1)\)</span>然后使用1x1卷积进行计算。</p></li><li><p>局部感知机：其中使用的卷积为深度卷积。</p><p>通过Locality Injection将局部感知机融合进通道感知机的过程：</p><p>首先明确局部感知机包含一个卷积操作和一次Batch Normalization，其中<span class="math inline">\(F\in \mathbb{R}^{s\times 1\times k\times k}\)</span>为卷积核，<span class="math inline">\(\mu,\sigma,\gamma,\beta \in \mathbb{R}^s\)</span>分别为BN操作中的均值、标准差、Scaling因子与bias。所以根据BN计算公式，有： <span class="math display">\[\begin{align}&amp;\gamma_i\cdot \frac{CONV(M,F,p)-\mu_i}{\sigma_i}+\beta_i\nonumber\\&amp;=CONV(M,\frac{\gamma_i\cdot F}{\sigma_i},p)-\frac{\gamma_i\cdot \mu_i}{\sigma_i}+\beta_i\end{align}\]</span> 假设存在<span class="math inline">\(F_{i,:,:,:}^\prime=\frac{\gamma_i\cdot F_{i,:,:,:}}{\sigma_i}\)</span>和<span class="math inline">\(b_{i}^\prime=-\frac{\gamma_i\cdot \mu_i}{\sigma_i}+\beta_i\)</span>，则公式（9）可简写为： <span class="math display">\[\begin{align}&amp;\gamma_i\cdot \frac{CONV(M,F,p)_{i,:,:,:}-\mu_i}{\sigma_i}+\beta_i\nonumber\\&amp;=CONV(M,F^\prime,p)_{i,:,:,:}+b^\prime,\forall 1\le i\le s\end{align}\]</span> 因此，通过公式（8）可以转换每个卷积操作，产生FC核并叠加到通道感知机中。</p></li></ul><h4 id="分层架构设计">分层架构设计</h4><figure><img src="https://s2.loli.net/2022/05/26/ZTG5gNX2Y3mkPfL.png" alt="图3" /><figcaption aria-hidden="true">图3</figcaption></figure><p>与常规MLP模型的初始大幅降采样后使用小Size计算不同，文中的模型采用卷积网络中常见的分层设计。对于输入图片，采用一个4x4且Stride=4的卷积进行4倍下采样，对于后面的每个阶段，采用Embedding层减半尺寸并加倍通道数。网络规模如下表所示：</p><p><img src="https://s2.loli.net/2022/05/26/aMtjfUQRC4nJLez.png" /></p><hr /><h2 id="实验与结果">实验与结果</h2><h3 id="图像分类">图像分类</h3><p><img src="https://s2.loli.net/2022/05/26/RcuUXNoFry3A9Qj.png" /></p><p>在相同的设置下，RepMLPNetT256在精度上比MLP-Mixer高出0.5%，而前者的FLOPs只有后者的1/4。在简单的训练方法下，ResMLP和MLP-Mixer明显下降，例如，在没有300个epoch DeiT式训练的情况下，ResMLP-S12的准确性下降了8.9%（76.6%→67.7%）。在FLOPs相当的情况下，MLP比CNN快，例如，RepMLPNet-D256的FLOPs比ResNeXt-101高，但运行速度是后者的1.6倍。</p><p><img src="https://s2.loli.net/2022/05/26/hUtjizP3mn7pkMG.png" /></p><h3 id="语义分割">语义分割</h3><p><img src="https://s2.loli.net/2022/05/26/rJ4j27YwX1dUD3G.png" /></p><h3 id="消融实验">消融实验</h3><p><img src="https://s2.loli.net/2022/05/26/O1g2HJiCbnPx8IR.png" /></p><p><img src="https://s2.loli.net/2022/05/26/EJmNuMchUnzjWQ5.png" /></p>]]></content>


        <categories>

            <category>论文笔记</category>

            <category>计算机视觉</category>

            <category>MLP</category>

        </categories>


        <tags>

            <tag>论文笔记</tag>

            <tag>计算机视觉</tag>

            <tag>MLP</tag>

        </tags>

    </entry>


    <entry>
        <title>Shunted Self-Attention via Multi-Scale Token Aggregation 阅读笔记</title>
        <link href="/2022/05/12/Shunted-Self-Attention-via-Multi-Scale-Token-Aggregation-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
        <url>
            /2022/05/12/Shunted-Self-Attention-via-Multi-Scale-Token-Aggregation-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/
        </url>

        <content type="html">
            <![CDATA[<h1 id="shunted-self-attention-via-multi-scale-token-aggregation">Shunted Self-Attention via Multi-Scale Token Aggregation</h1><blockquote><ul><li>Sucheng Ren，Daquan Zhou，Shengfeng He，Jiashi Feng，Xinchao Wang</li><li>National University of Singapore</li><li>South China University of Technology</li><li>ByteDance Inc.</li><li><a href="https://arxiv.org/abs/2111.15193">CVPR2022 Oral</a></li><li><a href="https://github.com/OliverRensu/Shunted-Transformer">Code</a></li></ul></blockquote><h2 id="背景">背景</h2><p><img src="https://s2.loli.net/2022/05/12/VRCmDf5Kvs91iBI.png" /></p><p><img src="https://s2.loli.net/2022/05/12/gqi9MBAa3TVZ7zj.png" /></p><p>传统ViT通过将图片切分为Patch后产生对应的Token然后渐进式的通过全局注意力提取Token特征，这种方法虽然能处理特征的长程依赖关系但是过多的Token会导致在处理复杂的全局关系时丢失细小物体的特征，并且当Token较多时，网络的参数量与计算量会进一步上升。例如上图中天花板的灯在基于ViT的PVT中并没有得到很好的注意，而文中提出的网络结构能够更好地关注细小物体的特征。</p><hr /><h2 id="method">Method</h2><p><img src="https://s2.loli.net/2022/05/12/NW1r84lL9KZnpYo.png" /></p><p>本文总体架构与ViT类似，均采用4阶段模式，辅以Patch Embedding、Linear Embedding操作，每个阶段进行一次下采样减小特征图分辨率并将通道数翻倍。</p><h3 id="shunted-transformer-block">Shunted Transformer Block</h3><h4 id="shunted-self-attention">Shunted Self-Attention</h4><p><img src="https://s2.loli.net/2022/05/12/PetuivHEWK3YXrN.png" /></p><p>与ViT一样，先将输入序列<span class="math inline">\(F\in \mathbb{R}^{h\times w\times c}\)</span>映射为<span class="math inline">\(Q,K,V\)</span>，然后再经过MHSA。但是与其不同的是本文的结构将<span class="math inline">\(K,V\)</span>的长度通过下采样的方式进行缩减以减少计算量并且以不同的长度捕获多尺度信息。其通过MTA（Multi-scale Token Aggregation）实现，公式如下： <span class="math display">\[\begin{align}&amp; Q_i=XW_I^Q\nonumber\\&amp; K_i,V_i=MTA(X,r_i)W_i^K,MTA(X,r_i)W_i^V\nonumber\\&amp; V_i=V_i+LE(V_i)\nonumber\\\end{align}\]</span> 其中<span class="math inline">\(i\)</span>为网络的第<span class="math inline">\(i\)</span>个阶段，<span class="math inline">\(MTA(\cdot,r_i)\)</span>为第<span class="math inline">\(i\)</span>阶段的采样率为<span class="math inline">\(r_i\)</span>的下采样MTA层（MTA的具体实现为一个Stride=<span class="math inline">\(r_i\)</span>的卷积层），<span class="math inline">\(W_i^Q,W_i^K,W_i^V\)</span>为第<span class="math inline">\(i\)</span>阶段的Linear Projection参数，<span class="math inline">\(LE(\cdot)\)</span>是一个对MTA进行局部增强的深度卷积层。最终的Self-Attention公式与ViT类似，即： <span class="math display">\[\begin{align}&amp; h_i=Softmax(\frac{Q_iK_i^T}{\sqrt{d_h}})V_i\nonumber\end{align}\]</span> <strong>通过上述公式可知，当<span class="math inline">\(r\)</span>增大时，<span class="math inline">\(K,V\)</span>中更多的Token得以被融合并且<span class="math inline">\(K,V\)</span>得以缩短，因此能在降低计算量的同时增强对大目标的捕获能力。相反地，当<span class="math inline">\(r\)</span>减小时，更多的细节可以被关注但也会增加计算成本。</strong></p><h4 id="detail-specific-feedforward-layers">Detail-specific Feedforward Layers</h4><p><img src="https://s2.loli.net/2022/05/12/IiwrSWF8EYH2hbv.png" /></p><p>为了进一步补充局部信息，本文的网络在前馈层添加一个Detail Specific模块，因此该网络的前馈层的表述公式为： <span class="math display">\[\begin{align}&amp; x^\prime=FC(x;\theta_1)\nonumber\\&amp; x^{\prime\prime}=FC(\sigma(x^\prime+DS(x^\prime;\theta));\theta_2)\nonumber\\\end{align}\]</span> 其中，<span class="math inline">\(DS(\cdot;\theta)\)</span>为使用参数<span class="math inline">\(\theta\)</span>的深度卷积层，<span class="math inline">\(\theta_1,\theta_2\)</span>分别为两个FC层的参数。</p><h3 id="patch-embedding">Patch Embedding</h3><p>论文<a href="https://arxiv.org/abs/2109.03810">Scaled ReLU Matters for Training Vision Transformers</a>表明在Patch Embedding阶段使用卷积操作可以得到高质量Token，其效果要比使用单个大Stride的无重叠卷积效果更优秀。</p><p>本文中使用两个卷积层和一个Projection层作为Patch Embedding，第一个卷积层为Stride=2的7x7卷积，第二个卷积层为Stride=1的3x3卷积，最终通过一个Stride=2的无重叠Projection层来产生长度为<span class="math inline">\(\frac{H}{4}\times\frac{W}{4}\)</span>的输入序列。</p><h3 id="architecture-details-and-variants">Architecture Details and Variants</h3><p>网络的输入为<span class="math inline">\(W\times H\times 3\)</span>的原始图片，经过Patch Embedding后产生<span class="math inline">\(C\)</span>个长度为<span class="math inline">\(\frac{H}{4}\times\frac{W}{4}\)</span>的Patch序列。每个阶段的Linear Projection部分由一个Stride=2的卷积层构成。每个阶段的最后特征图分辨率减半并且通道数翻倍。网络的几种配置规格如下表：</p><p><img src="https://s2.loli.net/2022/05/12/2pSsPZGaKndiuwM.png" /></p><hr /><h2 id="实验与结果">实验与结果</h2><h3 id="imagenet-1k-图像分类">ImageNet-1K 图像分类</h3><p><img src="https://s2.loli.net/2022/05/12/LoVabzUhgEGfIws.png" /></p><h3 id="coco-2017-目标检测">COCO 2017 目标检测</h3><p><img src="https://s2.loli.net/2022/05/12/SuJlrq7OtBYvGVx.png" /></p><p><img src="https://s2.loli.net/2022/05/13/WO2IbMU8wuK34ls.png" /></p><p>1xSchedule（12 epochs）的fine-tuning阶段将输入图的短边resize为800px，长边不大于1333px；而3xSchedule（36 epochs）的fine-tuning阶段采取多尺度训练策略，将较短的尺寸调整到480至800之间。</p><h3 id="adk20k-语义分割">ADK20K 语义分割</h3><p><img src="https://s2.loli.net/2022/05/12/i4CcmgoqJH3AsUh.png" /></p><p>下表为与使用MiT Backbone的SegFormer框架的对比：</p><p><img src="https://s2.loli.net/2022/05/12/Xj86hdbQI3GEfLF.png" /></p><h3 id="消融实验">消融实验</h3><h4 id="patch-embedding-1">Patch Embedding</h4><p><img src="https://s2.loli.net/2022/05/12/LkFm2KjZaHeMuQ4.png" /></p><p>其中Non-Overlap指ViT中使用的策略，而Overlap为Swin和PVT的Embedding策略。</p><h4 id="token-aggregation-function">Token Aggregation Function</h4><p><img src="https://s2.loli.net/2022/05/12/R5Mtv628UuVSNok.png" /></p><h4 id="detail-specific-feed-forward">Detail-specific Feed-Forward</h4><p><img src="https://s2.loli.net/2022/05/12/osnFl5bYRdqjMIk.png" /></p>]]></content>


        <categories>

            <category>论文笔记</category>

            <category>计算机视觉</category>

            <category>Transformer</category>

        </categories>


        <tags>

            <tag>论文笔记</tag>

            <tag>计算机视觉</tag>

            <tag>Transformer</tag>

        </tags>

    </entry>


    <entry>
        <title>MetaFormer is Actually What You Need for Vision 阅读笔记</title>
        <link href="/2022/05/12/MetaFormer-is-Actually-What-You-Need-for-Vision-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
        <url>/2022/05/12/MetaFormer-is-Actually-What-You-Need-for-Vision-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>

        <content type="html">
            <![CDATA[<h1 id="metaformer-is-actually-what-you-need-for-vision">MetaFormer is Actually What You Need for Vision</h1><blockquote><ul><li>Weihao Yu，Mi Luo，Pan Zhou，Chenyang Si，Yichen Zhou，Xinchao Wang，Jiashi Feng，Shuicheng Yan</li><li>Sea AI Lab</li><li>National University of Singapore</li><li><a href="https://arxiv.org/abs/2111.11418#">CVPR2022 Oral</a></li></ul></blockquote><h2 id="概述">概述</h2><p>如下图所示，视觉Transformer包含两个子残差块，第一个一般为Token Mixer模块（诸如MHSA、Spatial MLP等）用于长程特征提取融合，第二个一般为两层全连接层组成的MLP（其中第二层的长度比第一层大2倍/4倍）。</p><p><img src="https://s2.loli.net/2022/05/05/Ml3k7Qw6SmPFW4g.png" /></p><p>因为通过最近的论文发现传统Transformer的Token Mixer所使用的MHSA更换为Spatial MLP或者傅里叶变换块后仍然能维持很优秀的效果，因此本文对该结构进行了抽象，并且通过实验证明了：相比Token Mixer，整体的Transformer结构（即MetaFormer）才是最重要的。除此之外，本文也就该理念提出了一个将Token Mixer更换为AvgPool的更简单的模型，即PoolFormer，其在ImageNet分类、ADE20K语义分割和COCO目标检测数据集上均能以更少的参数量和更低的MAC（Memory Access Cost）取得与其他Transformer齐平甚至更高的水平。</p><hr /><h2 id="method">Method</h2><p>首先，提出了一个针对传统视觉Transformer Block的结构抽象，其公式如下： <span class="math display">\[\begin{align}&amp; X=InputEmb(I)，X\in \mathbb{R}^{N\times C}，即对输入I进行Embedding操作\nonumber\\&amp; Y=TokenMixer(Norm(X))+X，其中Norm可替换为LN、GN、BN\nonumber\\&amp; Z=\sigma(Norm(Y)W_1)W_2+Y，W_1\in \mathbb{R}^{C\times rC}，W_2\in \mathbb{R}^{rC\times C}\nonumber\\&amp; 其中W_1，W_2为可学习的MLP参数，r为MLP扩张率，\nonumber\\&amp; \sigma(\cdot)为激活函数，如GELU、ReLU等\nonumber\\\end{align}\]</span> 本文针对该抽象提出了一个自己的实现，即Token Mixer使用简单的且无可学习参数的AvgPool，构成PoolFormer Block，PyTorch代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Pooling</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pool_size=<span class="hljs-number">3</span></span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.pool = nn.AvgPool2d(<br>      pool_size, stride=<span class="hljs-number">1</span>,<br>      padding=pool_size//<span class="hljs-number">2</span>,<br>      count_include_pad=<span class="hljs-literal">False</span>,<br>    )<br>    <br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    [B, C, H, W] = x.shape</span><br><span class="hljs-string">    因为TokenMixer之外有一个残差连接，所以此处需要减去该连接。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> self.pool(x) - x<br></code></pre></td></tr></table></figure><p>AvgPool的stride=1，因此其本身具有一定的特征聚合能力，并且由于自身并没有可学习参数，所以相对于Self-Attention和Spatial MLP等机制，PoolFormer占用显存更少，可以处理非常长的Token序列。完整网络结构如下图所示：</p><p><img src="https://s2.loli.net/2022/05/05/x8yAfieaTrzcVL9.png" /></p><p>对于Small网络结构，Embedding维度对应至4阶段分别为64、128、320、512；而对于Medium网络结构，Embedding维度分别为96、192、384、768。</p><p>PoolFormer网络架构的几种配置如下所示：</p><p><img src="https://s2.loli.net/2022/05/05/aAo3rJEv6YyBmfe.png" /></p><hr /><h2 id="实验">实验</h2><h3 id="图像分类任务">图像分类任务</h3><p>数据集使用ImageNet-1K，其中包含1.3M张训练图片和5K张验证图片。</p><p><img src="https://s2.loli.net/2022/05/05/2GVhxDuUdZij4TK.png" /></p><p><img src="https://s2.loli.net/2022/05/05/r7RiazD3MAUgBbc.png" /></p><p>从表中可以看出PoolFormer与使用卷积、SA或Spatial MLP的网络相比，可以在相同甚至更少的参数量的前提下达到更好的效果，并且MAC也更低。</p><h3 id="目标检测任务">目标检测任务</h3><p>数据集使用COCO，包含118K张训练图片和5K张验证图片，PoolFormer作为Backbone应用在RetinaNet或者Mask R-CNN中。</p><p><img src="https://s2.loli.net/2022/05/05/Z4SFJRMiPGuA1Bs.png" /></p><p>表中数据表明应用PoolFormer Backbone的RetinaNet可以以更少的参数量在AP上击败以ResNet为Backbone的RetinaNet。</p><p><img src="https://s2.loli.net/2022/05/05/MUfKBVL4vmw8IhS.png" /></p><p>表中数据表明应用PoolFormer Backbone的Mask R-CNN可以以更少的参数量在AP上击败以ResNet为Backbone的Mask R-CNN。</p><h3 id="语义分割任务">语义分割任务</h3><p>该任务使用ADE20K数据集，其中训练图片20K张，验证图片2K张。</p><p><img src="https://s2.loli.net/2022/05/05/dQHrnjJSxwmMgKE.png" /></p><p>实验表明以PoolFormer为Backbone的Semantic FPN网络用更少的参数量达成了比PVT、ResNeXt、ResNet更好的分割精度。</p><h3 id="消融实验">消融实验</h3><p><img src="https://s2.loli.net/2022/05/05/JQX671PIWUnj4Co.png" /></p><p>对于Hybrid Stages：由于pool操作可以处理较长的序列信息而SA/Spatial MLP能够捕获全局信息，因此使用混合架构，即先使用PoolFormer处理长序列，随着靠后的Stage的序列长度缩短，改为使用Spatial FC作为Token Mixer，从表中可以看到该方法稍微增加了参数量但也带来了一定的性能提升。</p>]]></content>


        <categories>

            <category>论文笔记</category>

            <category>计算机视觉</category>

            <category>Transformer</category>

        </categories>


        <tags>

            <tag>论文笔记</tag>

            <tag>计算机视觉</tag>

            <tag>Transformer</tag>

        </tags>

    </entry>


</search>
